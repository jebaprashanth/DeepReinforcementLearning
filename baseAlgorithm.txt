---------------------------------------
actor and critic neural network models:
---------------------------------------

The actor network takes the portfolio state as input 
and outputs the portfolio weights.
The critic network takes the portfolio state and action as input 
and outputs the Q-value.

-------------------------------------------------
Deep Deterministic Policy Gradient (DDPG) agent -:
-------------------------------------------------

The agent class initializes the actor and critic networks 
and sets hyperparameters such as the learning rate, discount factor, 
and exploration noise.
The agent class includes functions for selecting actions, 
updating the networks using the DDPG algorithm, 
and storing and sampling experience replay data.


---------------------
portfolio environment:
---------------------

The portfolio environment class simulates the portfolio performance 
over a fixed time period based on the portfolio weights, asset prices, 
and transaction costs.
The environment class includes functions for resetting the environment, 
stepping through the environment by taking actions and receiving rewards, 
and calculating the portfolio value.

----------------------------------------------------
Train the DDPG agent using the portfolio environment:
----------------------------------------------------

The agent interacts with the environment by selecting actions 
based on the current state and updating the actor and critic 
networks using the DDPG algorithm.
The agent uses experience replay to sample random batches 
of past interactions and updates the networks using the replay buffer.
The agent's performance is evaluated using metrics 
such as the Sharpe ratio and cumulative return.


-------------------------------------------------
Use the trained agent to make portfolio decisions:
--------------------------------------------------

Once the agent is trained, it can be used to make portfolio decisions 
in real-time based on the current market conditions.
The agent selects actions by feeding the current portfolio state 
into the actor network and selecting the action with the highest predicted Q-value.


